{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "350c9686",
   "metadata": {},
   "source": [
    "# Face Detection from scracth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4300ce33",
   "metadata": {},
   "source": [
    "Importing libraries to gather images dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1416f548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02f08d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image_path=r\"C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\images\"\n",
    "number_images=90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3625160a",
   "metadata": {},
   "source": [
    "Capturing sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45d4fbc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "for i in range(number_images):\n",
    "    print('Collecting images-{}'.format(i))\n",
    "    ret,frame=cap.read()\n",
    "    if ret==True:\n",
    "        imgname=os.path.join(Image_path,f'{str(uuid.uuid1())}.jpg')\n",
    "        cv2.imwrite(imgname, frame)\n",
    "        cv2.imshow('frame',frame)\n",
    "    time.sleep(0.5)\n",
    "    if cv2.waitKey(1) & 0xFF==ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26510a67",
   "metadata": {},
   "source": [
    "# Labelling the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "258f1ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!labelme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5995df0a",
   "metadata": {},
   "source": [
    "Making new folders to put images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "718b2b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "newpath = r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\train' \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "newpath = r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\test' \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "newpath = r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\val' \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c36598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "newpath = r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\train\\images' \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "newpath = r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\train\\labels' \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "newpath = r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\test\\images' \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "newpath = r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\val\\images' \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "newpath = r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\test\\labels' \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "newpath = r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\val\\labels' \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47a0a2b",
   "metadata": {},
   "source": [
    "# Creating a tensorflow tensor(dataset) of all images and loading it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14101b75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ef48f27",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae832142",
   "metadata": {},
   "source": [
    "To create a dataset of all files matching a pattern, use tf.data.Dataset.list_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3901c5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images=tf.data.Dataset.list_files(r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\images\\*.jpg',shuffle=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29e0bc9",
   "metadata": {},
   "source": [
    "This will return the first element of the dataset as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc70b23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'C:\\\\Users\\\\Lenovo\\\\Desktop\\\\NEPR\\\\data\\\\images\\\\53ec8754-c30d-11ed-8b4d-8c8caa3dd0da.jpg'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7a61ec",
   "metadata": {},
   "source": [
    "Function to load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11359755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(x):\n",
    "    byte_img=tf.io.read_file(x) # reads content of the file\n",
    "    img=tf.io.decode_jpeg(byte_img) # Decode a JPEG-encoded image to a uint8 tensor.\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "504cf721",
   "metadata": {},
   "outputs": [],
   "source": [
    "images=images.map(load_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7c3282",
   "metadata": {},
   "source": [
    "Getting a list of all the images and randomly alloting them to the train, test and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a88bd87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Set up source and destination directories\n",
    "src_dir = Image_path\n",
    "train_dir = r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\train\\images'\n",
    "test_dir = r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\test\\images'\n",
    "val_dir = r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\val\\images'\n",
    "\n",
    "# Create destination directories if they don't exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# Get a list of all image filenames in the source directory\n",
    "all_images = os.listdir(src_dir)\n",
    "image_count = len(all_images)\n",
    "\n",
    "# Shuffle the list of image filenames\n",
    "random.shuffle(all_images)\n",
    "\n",
    "# Move images to train, test, and val directories\n",
    "for i, image in enumerate(all_images):\n",
    "    if i < 62:\n",
    "        dest_dir = train_dir\n",
    "    elif i < 76:\n",
    "        dest_dir = test_dir\n",
    "    else:\n",
    "        dest_dir = val_dir\n",
    "        \n",
    "    shutil.move(os.path.join(src_dir, image), os.path.join(dest_dir, image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5a58093",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=r'C:\\Users\\Lenovo\\Desktop\\NEPR'\n",
    "for folder in ['train','test','val']:\n",
    "    for file in os.listdir(os.path.join(x,'data',folder,'images')):\n",
    "        filename=file.split('.')[0]+'.json'\n",
    "        existing_filepath=os.path.join(x,'data','labels',filename)\n",
    "        if os.path.exists(existing_filepath):\n",
    "            new_filepath=os.path.join(x,'data',folder,'labels',filename)\n",
    "            os.replace(existing_filepath,new_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be5d604",
   "metadata": {},
   "source": [
    "# Data Augmentation using albumentations library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5abf779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as alb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b41e6be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentor = alb.Compose([alb.RandomCrop(width=450, height=450), \n",
    "                         alb.HorizontalFlip(p=0.5), \n",
    "                         alb.RandomBrightnessContrast(p=0.2),\n",
    "                         alb.RandomGamma(p=0.2), \n",
    "                         alb.RGBShift(p=0.2), \n",
    "                         alb.VerticalFlip(p=0.5)], \n",
    "                       bbox_params=alb.BboxParams(format='albumentations', \n",
    "                                                  label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49f5e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\aug_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39fe6f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "newpath = r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\aug_data\\train' \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "newpath = r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\aug_data\\train\\images' \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "newpath = r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\aug_data\\train\\labels' \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "newpath = r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\aug_data\\test' \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "newpath = r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\aug_data\\test\\images' \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "newpath = r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\aug_data\\test\\labels' \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "newpath = r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\aug_data\\val' \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "newpath = r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\aug_data\\val\\images' \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "newpath = r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\aug_data\\val\\labels' \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1901e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=r'C:\\Users\\Lenovo\\Desktop\\NEPR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eed5771b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for partition in ['train','test','val']: \n",
    "    for image in os.listdir(os.path.join(x,'data', partition, 'images')):\n",
    "        img = cv2.imread(os.path.join(x,'data', partition, 'images', image))\n",
    "\n",
    "        coords = [0,0,0.00001,0.00001]\n",
    "        label_path = os.path.join(x,'data', partition, 'labels', f'{image.split(\".\")[0]}.json')\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                label = json.load(f)\n",
    "\n",
    "            coords[0] = label['shapes'][0]['points'][0][0]\n",
    "            coords[1] = label['shapes'][0]['points'][0][1]\n",
    "            coords[2] = label['shapes'][0]['points'][1][0]\n",
    "            coords[3] = label['shapes'][0]['points'][1][1]\n",
    "            coords = list(np.divide(coords, [640,480,640,480]))\n",
    "\n",
    "        try: \n",
    "            for y in range(60):\n",
    "                augmented = augmentor(image=img, bboxes=[coords], class_labels=['face'])\n",
    "                cv2.imwrite(os.path.join(x,'data','aug_data', partition, 'images', f'{image.split(\".\")[0]}.{y}.jpg'), augmented['image'])\n",
    "\n",
    "                annotation = {}\n",
    "                annotation['image'] = image\n",
    "\n",
    "                if os.path.exists(label_path):\n",
    "                    if len(augmented['bboxes']) == 0: \n",
    "                        annotation['bbox'] = [0,0,0,0]\n",
    "                        annotation['class'] = 0 \n",
    "                    else: \n",
    "                        annotation['bbox'] = augmented['bboxes'][0]\n",
    "                        annotation['class'] = 1\n",
    "                else: \n",
    "                    annotation['bbox'] = [0,0,0,0]\n",
    "                    annotation['class'] = 0 \n",
    "\n",
    "\n",
    "                with open(os.path.join(x,'data','aug_data', partition, 'labels', f'{image.split(\".\")[0]}.{y}.json'), 'w') as f:\n",
    "                    json.dump(annotation, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6362391",
   "metadata": {},
   "source": [
    "# Images loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba4f2f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images=tf.data.Dataset.list_files(r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\aug_data\\train\\images\\*.jpg',shuffle=False)\n",
    "train_images=train_images.map(load_image)\n",
    "train_images=train_images.map(lambda x: tf.image.resize(x,(120,120)))\n",
    "train_images=train_images.map(lambda x: x/255)\n",
    "\n",
    "test_images=tf.data.Dataset.list_files(r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\aug_data\\test\\images\\*.jpg',shuffle=False)\n",
    "test_images=test_images.map(load_image)\n",
    "test_images=test_images.map(lambda x: tf.image.resize(x,(120,120)))\n",
    "test_images=test_images.map(lambda x: x/255)\n",
    "\n",
    "val_images=tf.data.Dataset.list_files(r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\aug_data\\val\\images\\*.jpg',shuffle=False)\n",
    "val_images=val_images.map(load_image)\n",
    "val_images=val_images.map(lambda x: tf.image.resize(x,(120,120)))\n",
    "val_images=val_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a315f",
   "metadata": {},
   "source": [
    "# Label Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f78b385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(label_path):\n",
    "    with open(label_path.numpy(),'r',encoding=\"utf-8\") as f:\n",
    "        label=json.load(f)\n",
    "    return [label['class']], label['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fcafc05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels=tf.data.Dataset.list_files(r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\aug_data\\train\\labels\\*.json',shuffle=False)\n",
    "train_labels=train_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bca2fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels=tf.data.Dataset.list_files(r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\aug_data\\test\\labels\\*.json',shuffle=False)\n",
    "test_labels=test_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "980502e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels=tf.data.Dataset.list_files(r'C:\\Users\\Lenovo\\Desktop\\NEPR\\data\\aug_data\\val\\labels\\*.json',shuffle=False)\n",
    "val_labels=val_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f8b545c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3720, 3720, 840, 840, 840, 840)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_images), len(train_labels), len(test_images), len(test_labels), len(val_images), len(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dce622",
   "metadata": {},
   "source": [
    "# Calling the images in batches so that the GPU isn't fully consumed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49e8029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.data.Dataset.zip((train_images, train_labels))\n",
    "train = train.shuffle(5000)\n",
    "train = train.batch(8)\n",
    "train = train.prefetch(4)\n",
    "\n",
    "test = tf.data.Dataset.zip((test_images, test_labels))\n",
    "test = test.shuffle(1300)\n",
    "test = test.batch(8)\n",
    "test = test.prefetch(4)\n",
    "\n",
    "val = tf.data.Dataset.zip((val_images, val_labels))\n",
    "val = val.shuffle(1000)\n",
    "val = val.batch(8)\n",
    "val = val.prefetch(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f167bdb0",
   "metadata": {},
   "source": [
    "# Data Visualization of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "559dddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = train.as_numpy_iterator()\n",
    "res = data_samples.next()\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx in range(4): \n",
    "    sample_image = res[0][idx]\n",
    "    sample_coords = res[1][1][idx]\n",
    "    \n",
    "    cv2.rectangle(sample_image, \n",
    "                  tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
    "                  tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)), \n",
    "                        (255,0,0), 2)\n",
    "\n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca54a4a9",
   "metadata": {},
   "source": [
    "# Using VGG16 as the backbone, we create a model for object detection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a774ab",
   "metadata": {},
   "source": [
    "which detects face as the only object and remove the last layer of the VGG16 to use it only for 1 object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0de1fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Dense, GlobalMaxPooling2D\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec57f14",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64fe5389",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg=VGG16(include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45899c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_layer=Input(shape=(120,120,3))\n",
    "    \n",
    "    vgg=VGG16(include_top=False)(input_layer)\n",
    "    f1=GlobalMaxPooling2D()(vgg)\n",
    "    \n",
    "    #Classification Model\n",
    "    class1=Dense(2048,activation='relu')(f1)\n",
    "    class2=Dense(1,activation='sigmoid')(class1)\n",
    "    \n",
    "    #Bounding Box Model\n",
    "    f2=GlobalMaxPooling2D()(vgg)\n",
    "    regress1=Dense(2048, activation='relu')(f2)\n",
    "    regress2=Dense(4,activation='sigmoid')(regress1)\n",
    "    \n",
    "    facetracker=Model(inputs=input_layer, outputs=[class2,regress2])\n",
    "    \n",
    "    return facetracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5aa2f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "facet=build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1aa50f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 120, 120, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vgg16 (Functional)              (None, None, None, 5 14714688    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d (GlobalMax (None, 512)          0           vgg16[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_1 (GlobalM (None, 512)          0           vgg16[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2048)         1050624     global_max_pooling2d[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2048)         1050624     global_max_pooling2d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            2049        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4)            8196        dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 16,826,181\n",
      "Trainable params: 16,826,181\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "facet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f8564212",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c975854",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, coords = facet.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53ae9eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.6562486 ],\n",
       "        [0.55615944],\n",
       "        [0.60473657],\n",
       "        [0.5542188 ],\n",
       "        [0.5566731 ],\n",
       "        [0.6106665 ],\n",
       "        [0.640695  ],\n",
       "        [0.5560473 ]], dtype=float32),\n",
       " array([[0.3851599 , 0.55994153, 0.54812646, 0.40848655],\n",
       "        [0.4544277 , 0.45799834, 0.60939586, 0.5412904 ],\n",
       "        [0.37409174, 0.597939  , 0.584196  , 0.4253002 ],\n",
       "        [0.46068138, 0.49632058, 0.5696219 , 0.5194828 ],\n",
       "        [0.39171106, 0.51848817, 0.6187348 , 0.5366577 ],\n",
       "        [0.44175845, 0.5965603 , 0.5929734 , 0.5009631 ],\n",
       "        [0.46395427, 0.6194469 , 0.52898335, 0.45451048],\n",
       "        [0.4708101 , 0.5963392 , 0.5684964 , 0.48080784]], dtype=float32))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes, coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "30bc40e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch=len(train)\n",
    "lr_decay=(1/0.75-1)/batches_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8df76a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=tf.keras.optimizers.Adam(learning_rate=0.0001, decay=lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "52a360d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def localization_loss(y_true,y_hat):\n",
    "    delta_coord=tf.reduce_sum(tf.square(y_true[:,:2]-y_hat[:,:2]))\n",
    "    \n",
    "    h_true=y_true[:,3]-y_true[:,1]\n",
    "    w_true=y_true[:,2]-y_true[:,0]\n",
    "    \n",
    "    h_pred=y_hat[:,3]-y_hat[:,1]\n",
    "    w_pred=y_hat[:,2]-y_hat[:,0]\n",
    "    \n",
    "    delta_size=tf.reduce_sum(tf.square(w_true-w_pred)+tf.square(h_true-h_pred))\n",
    "    \n",
    "    return delta_coord+delta_size\n",
    "\n",
    "\n",
    "classloss=tf.keras.losses.BinaryCrossentropy()\n",
    "regressloss=localization_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb1797f",
   "metadata": {},
   "source": [
    "# Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "412ecc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceTracker(Model):\n",
    "    def __init__(self, facetracker, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model=facetracker\n",
    "        \n",
    "    def compile(self, opt, classloss, localizationloss, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.closs=classloss\n",
    "        self.llos=localizationloss\n",
    "        self.opt=opt\n",
    "    \n",
    "    def train_step(self, batch, **kwargs):\n",
    "        X,y= batch\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            classes, coords=self.model(X, training=True)\n",
    "            \n",
    "            batch_classloss=self.closs(y[0], classes)\n",
    "            batch_localizationloss=self.llos(tf.cast(y[1],tf.float32), coords)\n",
    "            \n",
    "            total_loss=batch_localizationloss+0.5*batch_classloss\n",
    "            \n",
    "            grad=tape.gradient(total_loss, self.model.trainable_variables)\n",
    "            \n",
    "        opt.apply_gradients(zip(grad, self.model.trainable_variables))\n",
    "        \n",
    "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n",
    "    \n",
    "    def test_step(self, batch, **kwargs):\n",
    "        \n",
    "        X,y=batch\n",
    "        classes, coords=self.model(X, training=True)\n",
    "            \n",
    "        batch_classloss=self.closs(y[0], classes)\n",
    "        batch_localizationloss=self.llos(tf.cast(y[1],tf.float32), coords)\n",
    "        total_loss=batch_localizationloss+0.5*batch_classloss\n",
    "        \n",
    "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n",
    "\n",
    "    \n",
    "    def call(self, X, **kwargs):\n",
    "        return self.model(X, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a8a78ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=FaceTracker(facet)\n",
    "model.compile(opt, classloss, regressloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3dd35f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir='logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d866466",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback=tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "95c9de8a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "465/465 [==============================] - 97s 180ms/step - total_loss: 0.2120 - class_loss: 0.0684 - regress_loss: 0.1778 - val_total_loss: 0.0126 - val_class_loss: 8.8519e-04 - val_regress_loss: 0.0122\n",
      "Epoch 2/40\n",
      "465/465 [==============================] - 78s 162ms/step - total_loss: 0.0247 - class_loss: 0.0075 - regress_loss: 0.0210 - val_total_loss: 0.0046 - val_class_loss: 2.5608e-04 - val_regress_loss: 0.0045\n",
      "Epoch 3/40\n",
      "465/465 [==============================] - 77s 160ms/step - total_loss: 0.0036 - class_loss: 8.9843e-05 - regress_loss: 0.0035 - val_total_loss: 0.0038 - val_class_loss: 1.2471e-04 - val_regress_loss: 0.0037\n",
      "Epoch 4/40\n",
      "465/465 [==============================] - 77s 160ms/step - total_loss: 0.0025 - class_loss: 4.2085e-05 - regress_loss: 0.0025 - val_total_loss: 0.0056 - val_class_loss: 5.7813e-05 - val_regress_loss: 0.0056\n",
      "Epoch 5/40\n",
      "465/465 [==============================] - 77s 160ms/step - total_loss: 0.0022 - class_loss: 2.6474e-05 - regress_loss: 0.0022 - val_total_loss: 0.0045 - val_class_loss: 1.4425e-05 - val_regress_loss: 0.0045\n",
      "Epoch 6/40\n",
      "465/465 [==============================] - 77s 160ms/step - total_loss: 0.0020 - class_loss: 1.8613e-05 - regress_loss: 0.0020 - val_total_loss: 0.0030 - val_class_loss: 1.7137e-05 - val_regress_loss: 0.0030\n",
      "Epoch 7/40\n",
      "465/465 [==============================] - 77s 160ms/step - total_loss: 0.0017 - class_loss: 1.3048e-05 - regress_loss: 0.0017 - val_total_loss: 0.0014 - val_class_loss: 1.1921e-05 - val_regress_loss: 0.0014\n",
      "Epoch 8/40\n",
      "465/465 [==============================] - 77s 160ms/step - total_loss: 0.0016 - class_loss: 1.0161e-05 - regress_loss: 0.0016 - val_total_loss: 0.0035 - val_class_loss: 1.2249e-05 - val_regress_loss: 0.0035\n",
      "Epoch 9/40\n",
      "465/465 [==============================] - 77s 160ms/step - total_loss: 0.0015 - class_loss: 7.6050e-06 - regress_loss: 0.0015 - val_total_loss: 0.0029 - val_class_loss: 8.2255e-06 - val_regress_loss: 0.0029\n",
      "Epoch 10/40\n",
      "465/465 [==============================] - 77s 160ms/step - total_loss: 0.0015 - class_loss: 7.0764e-06 - regress_loss: 0.0015 - val_total_loss: 0.0030 - val_class_loss: 6.9143e-06 - val_regress_loss: 0.0030\n",
      "Epoch 11/40\n",
      "465/465 [==============================] - 77s 160ms/step - total_loss: 0.0014 - class_loss: 5.4810e-06 - regress_loss: 0.0014 - val_total_loss: 0.0016 - val_class_loss: 1.0953e-05 - val_regress_loss: 0.0016\n",
      "Epoch 12/40\n",
      "465/465 [==============================] - 77s 160ms/step - total_loss: 0.0013 - class_loss: 4.6523e-06 - regress_loss: 0.0013 - val_total_loss: 0.0040 - val_class_loss: 7.2569e-06 - val_regress_loss: 0.0040\n",
      "Epoch 13/40\n",
      "465/465 [==============================] - 77s 160ms/step - total_loss: 0.0012 - class_loss: 3.8179e-06 - regress_loss: 0.0012 - val_total_loss: 0.0026 - val_class_loss: 1.0431e-06 - val_regress_loss: 0.0026\n",
      "Epoch 14/40\n",
      "465/465 [==============================] - 77s 160ms/step - total_loss: 0.0012 - class_loss: 3.4520e-06 - regress_loss: 0.0012 - val_total_loss: 0.0029 - val_class_loss: 5.8860e-06 - val_regress_loss: 0.0029\n",
      "Epoch 15/40\n",
      "465/465 [==============================] - 77s 160ms/step - total_loss: 0.0012 - class_loss: 2.8848e-06 - regress_loss: 0.0012 - val_total_loss: 0.0017 - val_class_loss: 6.0648e-06 - val_regress_loss: 0.0017\n",
      "Epoch 16/40\n",
      "465/465 [==============================] - 77s 160ms/step - total_loss: 9.5687e-04 - class_loss: 2.2501e-06 - regress_loss: 9.5575e-04 - val_total_loss: 0.0030 - val_class_loss: 4.0233e-07 - val_regress_loss: 0.0030\n",
      "Epoch 17/40\n",
      "465/465 [==============================] - 77s 160ms/step - total_loss: 9.1046e-04 - class_loss: 1.9398e-06 - regress_loss: 9.0949e-04 - val_total_loss: 0.0031 - val_class_loss: 5.5134e-07 - val_regress_loss: 0.0031\n",
      "Epoch 18/40\n",
      "465/465 [==============================] - 77s 160ms/step - total_loss: 8.6659e-04 - class_loss: 1.8107e-06 - regress_loss: 8.6568e-04 - val_total_loss: 0.0022 - val_class_loss: 2.3842e-06 - val_regress_loss: 0.0022\n",
      "Epoch 19/40\n",
      "465/465 [==============================] - 77s 160ms/step - total_loss: 7.6886e-04 - class_loss: 1.5128e-06 - regress_loss: 7.6811e-04 - val_total_loss: 0.0027 - val_class_loss: 1.3858e-06 - val_regress_loss: 0.0027\n",
      "Epoch 20/40\n",
      "465/465 [==============================] - 77s 160ms/step - total_loss: 7.5505e-04 - class_loss: 1.1643e-06 - regress_loss: 7.5447e-04 - val_total_loss: 0.0027 - val_class_loss: 1.2815e-06 - val_regress_loss: 0.0027\n",
      "Epoch 21/40\n",
      "465/465 [==============================] - 77s 160ms/step - total_loss: 7.1095e-04 - class_loss: 9.6700e-07 - regress_loss: 7.1047e-04 - val_total_loss: 0.0019 - val_class_loss: 1.3560e-06 - val_regress_loss: 0.0019\n",
      "Epoch 22/40\n",
      "465/465 [==============================] - 77s 161ms/step - total_loss: 6.1555e-04 - class_loss: 7.9858e-07 - regress_loss: 6.1515e-04 - val_total_loss: 0.0025 - val_class_loss: 1.6689e-06 - val_regress_loss: 0.0025\n",
      "Epoch 23/40\n",
      "465/465 [==============================] - 77s 160ms/step - total_loss: 6.1229e-04 - class_loss: 6.6693e-07 - regress_loss: 6.1195e-04 - val_total_loss: 0.0028 - val_class_loss: 1.6838e-06 - val_regress_loss: 0.0028\n",
      "Epoch 24/40\n",
      "465/465 [==============================] - 77s 160ms/step - total_loss: 5.1796e-04 - class_loss: 6.2494e-07 - regress_loss: 5.1764e-04 - val_total_loss: 0.0036 - val_class_loss: 6.4075e-07 - val_regress_loss: 0.0036\n",
      "Epoch 25/40\n",
      "465/465 [==============================] - 171s 363ms/step - total_loss: 4.8106e-04 - class_loss: 4.9997e-07 - regress_loss: 4.8081e-04 - val_total_loss: 0.0029 - val_class_loss: 8.9407e-07 - val_regress_loss: 0.0029\n",
      "Epoch 26/40\n",
      "465/465 [==============================] - 76s 157ms/step - total_loss: 5.0246e-04 - class_loss: 4.4024e-07 - regress_loss: 5.0224e-04 - val_total_loss: 0.0025 - val_class_loss: 1.4454e-06 - val_regress_loss: 0.0025\n",
      "Epoch 27/40\n",
      "465/465 [==============================] - 76s 157ms/step - total_loss: 4.6117e-04 - class_loss: 3.8638e-07 - regress_loss: 4.6098e-04 - val_total_loss: 0.0038 - val_class_loss: 9.0897e-07 - val_regress_loss: 0.0038\n",
      "Epoch 28/40\n",
      "465/465 [==============================] - 76s 157ms/step - total_loss: 3.9667e-04 - class_loss: 3.3106e-07 - regress_loss: 3.9650e-04 - val_total_loss: 0.0022 - val_class_loss: 1.4901e-08 - val_regress_loss: 0.0022\n",
      "Epoch 29/40\n",
      "465/465 [==============================] - 75s 157ms/step - total_loss: 3.6527e-04 - class_loss: 2.6176e-07 - regress_loss: 3.6514e-04 - val_total_loss: 0.0041 - val_class_loss: 0.0000e+00 - val_regress_loss: 0.0041\n",
      "Epoch 30/40\n",
      "465/465 [==============================] - 75s 157ms/step - total_loss: 3.2060e-04 - class_loss: 2.2582e-07 - regress_loss: 3.2049e-04 - val_total_loss: 0.0015 - val_class_loss: 5.0664e-07 - val_regress_loss: 0.0015\n",
      "Epoch 31/40\n",
      "465/465 [==============================] - 75s 157ms/step - total_loss: 3.1768e-04 - class_loss: 2.2657e-07 - regress_loss: 3.1757e-04 - val_total_loss: 0.0029 - val_class_loss: 0.0000e+00 - val_regress_loss: 0.0029\n",
      "Epoch 32/40\n",
      "465/465 [==============================] - 75s 157ms/step - total_loss: 2.7995e-04 - class_loss: 1.7531e-07 - regress_loss: 2.7986e-04 - val_total_loss: 0.0051 - val_class_loss: 1.3411e-07 - val_regress_loss: 0.0051\n",
      "Epoch 33/40\n",
      "465/465 [==============================] - 76s 157ms/step - total_loss: 3.0139e-04 - class_loss: 1.6756e-07 - regress_loss: 3.0131e-04 - val_total_loss: 0.0014 - val_class_loss: 2.2352e-07 - val_regress_loss: 0.0014\n",
      "Epoch 34/40\n",
      "465/465 [==============================] - 75s 157ms/step - total_loss: 2.6372e-04 - class_loss: 1.3317e-07 - regress_loss: 2.6365e-04 - val_total_loss: 0.0040 - val_class_loss: 1.9372e-07 - val_regress_loss: 0.0040\n",
      "Epoch 35/40\n",
      "465/465 [==============================] - 75s 157ms/step - total_loss: 2.3780e-04 - class_loss: 1.1056e-07 - regress_loss: 2.3775e-04 - val_total_loss: 0.0025 - val_class_loss: 7.4506e-08 - val_regress_loss: 0.0025\n",
      "Epoch 36/40\n",
      "465/465 [==============================] - 75s 157ms/step - total_loss: 2.3068e-04 - class_loss: 9.1486e-08 - regress_loss: 2.3063e-04 - val_total_loss: 0.0026 - val_class_loss: 1.0431e-07 - val_regress_loss: 0.0026\n",
      "Epoch 37/40\n",
      "465/465 [==============================] - 75s 157ms/step - total_loss: 1.9861e-04 - class_loss: 8.4323e-08 - regress_loss: 1.9857e-04 - val_total_loss: 0.0025 - val_class_loss: 2.8312e-07 - val_regress_loss: 0.0025\n",
      "Epoch 38/40\n",
      "465/465 [==============================] - 75s 157ms/step - total_loss: 2.0172e-04 - class_loss: 6.7295e-08 - regress_loss: 2.0168e-04 - val_total_loss: 0.0020 - val_class_loss: 2.2352e-07 - val_regress_loss: 0.0020\n",
      "Epoch 39/40\n",
      "465/465 [==============================] - 75s 157ms/step - total_loss: 1.8083e-04 - class_loss: 5.8422e-08 - regress_loss: 1.8080e-04 - val_total_loss: 0.0022 - val_class_loss: 2.9802e-08 - val_regress_loss: 0.0022\n",
      "Epoch 40/40\n",
      "465/465 [==============================] - 76s 157ms/step - total_loss: 1.9075e-04 - class_loss: 5.3929e-08 - regress_loss: 1.9073e-04 - val_total_loss: 5.6803e-04 - val_class_loss: 1.0431e-07 - val_regress_loss: 5.6798e-04\n"
     ]
    }
   ],
   "source": [
    "hist=model.fit(train, epochs=40, validation_data=val, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ab4f5313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_loss': [0.02023053728044033,\n",
       "  0.00592361856251955,\n",
       "  0.0030834004282951355,\n",
       "  0.0019203294068574905,\n",
       "  0.0018834942020475864,\n",
       "  0.0030586475040763617,\n",
       "  0.0016115113394334912,\n",
       "  0.0012184057850390673,\n",
       "  0.0011218368308618665,\n",
       "  0.001892229076474905,\n",
       "  0.002018890343606472,\n",
       "  0.001703345449641347,\n",
       "  0.0012205207021906972,\n",
       "  0.001116050872951746,\n",
       "  0.0012678918428719044,\n",
       "  0.0005508808535523713,\n",
       "  0.0012820172123610973,\n",
       "  0.0008360611973330379,\n",
       "  0.0007341080927290022,\n",
       "  0.001308199716731906,\n",
       "  0.000823320122435689,\n",
       "  0.0004846368101425469,\n",
       "  0.0008018279331736267,\n",
       "  0.00039179850136861205,\n",
       "  0.000366563763236627,\n",
       "  0.0003935748536605388,\n",
       "  0.0007421282352879643,\n",
       "  0.00048128305934369564,\n",
       "  0.00020339799812063575,\n",
       "  0.00046295864740386605,\n",
       "  0.0001361620961688459,\n",
       "  0.00026447506388649344,\n",
       "  0.00029362994246184826,\n",
       "  0.0002729453844949603,\n",
       "  0.00029791519045829773,\n",
       "  0.0003897578571923077,\n",
       "  8.705060463398695e-05,\n",
       "  7.389362872345373e-05,\n",
       "  0.00010105592082254589,\n",
       "  0.00016014641732908785],\n",
       " 'class_loss': [0.0026840229984372854,\n",
       "  6.623478839173913e-05,\n",
       "  1.8999555322807282e-05,\n",
       "  6.899271738802781e-06,\n",
       "  3.5464868233248126e-06,\n",
       "  7.63698153605219e-06,\n",
       "  1.2398014405334834e-05,\n",
       "  5.09384844917804e-05,\n",
       "  4.7833173084654845e-06,\n",
       "  1.7285394733335124e-06,\n",
       "  6.973832569201477e-06,\n",
       "  1.408161551808007e-06,\n",
       "  4.470351484542334e-07,\n",
       "  3.546496827766532e-06,\n",
       "  7.122900569811463e-06,\n",
       "  2.533198539822479e-07,\n",
       "  1.803045961423777e-06,\n",
       "  1.6987389699352207e-06,\n",
       "  3.1292455560105736e-07,\n",
       "  1.3411049337719305e-07,\n",
       "  -0.0,\n",
       "  2.980232594040899e-08,\n",
       "  9.238730172000942e-07,\n",
       "  5.811462528981792e-07,\n",
       "  8.046640687098261e-07,\n",
       "  7.4505820180093e-08,\n",
       "  2.0861638461155962e-07,\n",
       "  4.0233155118585273e-07,\n",
       "  2.0861631355728605e-07,\n",
       "  8.195652299036738e-07,\n",
       "  2.816348796841339e-06,\n",
       "  7.67411165725207e-07,\n",
       "  1.4901162970204496e-08,\n",
       "  2.2351761685968086e-07,\n",
       "  2.6822112886293326e-07,\n",
       "  3.129247545530234e-07,\n",
       "  1.3411052179890248e-07,\n",
       "  -0.0,\n",
       "  -0.0,\n",
       "  -0.0],\n",
       " 'regress_loss': [0.018888525664806366,\n",
       "  0.005890501197427511,\n",
       "  0.003073900705203414,\n",
       "  0.0019168797880411148,\n",
       "  0.0018817209638655186,\n",
       "  0.003054829081520438,\n",
       "  0.0016053123399615288,\n",
       "  0.0011929365573450923,\n",
       "  0.001119445194490254,\n",
       "  0.0018913648091256618,\n",
       "  0.0020154034718871117,\n",
       "  0.0017026413697749376,\n",
       "  0.0012202971847727895,\n",
       "  0.0011142776347696781,\n",
       "  0.001264330348931253,\n",
       "  0.0005507541936822236,\n",
       "  0.0012811156921088696,\n",
       "  0.0008352118311449885,\n",
       "  0.0007339516305364668,\n",
       "  0.0013081326615065336,\n",
       "  0.000823320122435689,\n",
       "  0.00048462190898135304,\n",
       "  0.0008013659971766174,\n",
       "  0.000391507928725332,\n",
       "  0.0003661614318843931,\n",
       "  0.0003935376007575542,\n",
       "  0.0007420239271596074,\n",
       "  0.0004810818936675787,\n",
       "  0.00020329368999227881,\n",
       "  0.00046254886547103524,\n",
       "  0.00013475392188411206,\n",
       "  0.00026409135898575187,\n",
       "  0.00029362249188125134,\n",
       "  0.00027283362578600645,\n",
       "  0.0002977810800075531,\n",
       "  0.0003896013949997723,\n",
       "  8.698354940861464e-05,\n",
       "  7.389362872345373e-05,\n",
       "  0.00010105592082254589,\n",
       "  0.00016014641732908785],\n",
       " 'val_total_loss': [0.012622611597180367,\n",
       "  0.004596631973981857,\n",
       "  0.003770900657400489,\n",
       "  0.005581675563007593,\n",
       "  0.004464345518499613,\n",
       "  0.0029977536760270596,\n",
       "  0.0013921413337811828,\n",
       "  0.003489833092316985,\n",
       "  0.0028910120017826557,\n",
       "  0.003048754995688796,\n",
       "  0.0015557230217382312,\n",
       "  0.003986360505223274,\n",
       "  0.0026229515206068754,\n",
       "  0.0029189768247306347,\n",
       "  0.0016837011789903045,\n",
       "  0.0030275878962129354,\n",
       "  0.003072652965784073,\n",
       "  0.0021561936009675264,\n",
       "  0.0026595592498779297,\n",
       "  0.0027288179844617844,\n",
       "  0.0019142249366268516,\n",
       "  0.0025390926748514175,\n",
       "  0.002779245376586914,\n",
       "  0.003594348207116127,\n",
       "  0.002859632484614849,\n",
       "  0.002509515034034848,\n",
       "  0.0038396373856812716,\n",
       "  0.0022226988803595304,\n",
       "  0.004105662927031517,\n",
       "  0.0014861756935715675,\n",
       "  0.0029001205693930387,\n",
       "  0.005082719027996063,\n",
       "  0.0014238303992897272,\n",
       "  0.0039755599573254585,\n",
       "  0.002476392313838005,\n",
       "  0.0025791474618017673,\n",
       "  0.002474659588187933,\n",
       "  0.001975990366190672,\n",
       "  0.002178241265937686,\n",
       "  0.000568028655834496],\n",
       " 'val_class_loss': [0.0008851930033415556,\n",
       "  0.0002560834400355816,\n",
       "  0.0001247125182999298,\n",
       "  5.781336221843958e-05,\n",
       "  1.4424605979002081e-05,\n",
       "  1.713727215246763e-05,\n",
       "  1.1921169061679393e-05,\n",
       "  1.224901461682748e-05,\n",
       "  8.225522833527066e-06,\n",
       "  6.91425566401449e-06,\n",
       "  1.0952572665701155e-05,\n",
       "  7.25694826542167e-06,\n",
       "  1.0430832162455772e-06,\n",
       "  5.8860177887254395e-06,\n",
       "  6.064848548703594e-06,\n",
       "  4.023315227641433e-07,\n",
       "  5.513435326065519e-07,\n",
       "  2.38419852394145e-06,\n",
       "  1.3858150396117708e-06,\n",
       "  1.2815046375180827e-06,\n",
       "  1.3560090792452684e-06,\n",
       "  1.6689355106791481e-06,\n",
       "  1.6838364444993204e-06,\n",
       "  6.407507839867321e-07,\n",
       "  8.940718316807761e-07,\n",
       "  1.4454196843871614e-06,\n",
       "  9.089719696930842e-07,\n",
       "  1.4901162970204496e-08,\n",
       "  -0.0,\n",
       "  5.066405037723598e-07,\n",
       "  -0.0,\n",
       "  1.3411052179890248e-07,\n",
       "  2.2351761685968086e-07,\n",
       "  1.937152518394214e-07,\n",
       "  7.450582728552035e-08,\n",
       "  1.0430817098949774e-07,\n",
       "  2.8312231847849034e-07,\n",
       "  2.2351753159455257e-07,\n",
       "  2.980232594040899e-08,\n",
       "  1.0430814967321567e-07],\n",
       " 'val_regress_loss': [0.012180015444755554,\n",
       "  0.004468590021133423,\n",
       "  0.003708544420078397,\n",
       "  0.005552768707275391,\n",
       "  0.0044571333564817905,\n",
       "  0.00298918504267931,\n",
       "  0.0013861807528883219,\n",
       "  0.00348370848223567,\n",
       "  0.0028868992812931538,\n",
       "  0.0030452979262918234,\n",
       "  0.0015502467285841703,\n",
       "  0.003982732072472572,\n",
       "  0.0026224299799650908,\n",
       "  0.0029160338453948498,\n",
       "  0.0016806687926873565,\n",
       "  0.0030273867305368185,\n",
       "  0.0030723772943019867,\n",
       "  0.0021550015080720186,\n",
       "  0.0026588663458824158,\n",
       "  0.002728177234530449,\n",
       "  0.0019135469337925315,\n",
       "  0.002538258209824562,\n",
       "  0.0027784034609794617,\n",
       "  0.0035940278321504593,\n",
       "  0.0028591854497790337,\n",
       "  0.0025087923277169466,\n",
       "  0.003839182900264859,\n",
       "  0.0022226914297789335,\n",
       "  0.004105662927031517,\n",
       "  0.0014859223738312721,\n",
       "  0.0029001205693930387,\n",
       "  0.005082651972770691,\n",
       "  0.0014237186405807734,\n",
       "  0.0039754630997776985,\n",
       "  0.0024763550609350204,\n",
       "  0.002579095307737589,\n",
       "  0.0024745180271565914,\n",
       "  0.001975878607481718,\n",
       "  0.002178226364776492,\n",
       "  0.0005679765017703176]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1df099",
   "metadata": {},
   "source": [
    "# Visualization of prediction on data from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8771de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test.as_numpy_iterator()\n",
    "test_sample = test_data.next()\n",
    "yhat = facet.predict(test_sample[0])\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx in range(4): \n",
    "    sample_image = test_sample[0][idx]\n",
    "    sample_coords = yhat[1][idx]\n",
    "    \n",
    "    if yhat[0][idx] > 0.9:\n",
    "        cv2.rectangle(sample_image, \n",
    "                      tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "    \n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac74dcf1",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c02e0d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1a89a198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "facet.save('facedetect.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41535ad9",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e47e248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "facetracker = load_model('facedetect.h5')\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df52bc51",
   "metadata": {},
   "source": [
    "Using our front camera we can now detect face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d83625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    _ , frame = cap.read()\n",
    "    frame = frame[50:500, 50:500,:]\n",
    "    \n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    resized = tf.image.resize(rgb, (120,120))\n",
    "    \n",
    "    yhat = facetracker.predict(np.expand_dims(resized/255,0))\n",
    "    sample_coords = yhat[1][0]\n",
    "    \n",
    "    if yhat[0] > 0.5: \n",
    "        # Controls the main rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.multiply(sample_coords[:2], [450,450]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [450,450]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "        # Controls the label rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int), \n",
    "                                    [0,-30])),\n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                    [80,0])), \n",
    "                            (255,0,0), -1)\n",
    "        \n",
    "        # Controls the text rendered\n",
    "        cv2.putText(frame, 'Face', tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                               [0,-5])),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow('Facetrack', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512bc784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
